{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Classification with Keras\n",
    "\n",
    "Keras exists to make coding deep neural networks simpler. To demonstrate just how easy it is, youâ€™re going to use Keras to build a convolutional neural network in a few dozen lines of code.\n",
    "\n",
    "Youâ€™ll be connecting the concepts from the previous lessons to the methods that Keras provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The network you'll build with Keras is similar to the example that you can find in Kerasâ€™s GitHub repository that builds out a [convolutional neural network for MNIST](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py). \n",
    "\n",
    "However, instead of using the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, you're going to use the [German Traffic Sign Recognition Benchmark](http://benchmark.ini.rub.de/?section=gtsrb&subsection=news) dataset that you've used previously.\n",
    "\n",
    "You can download pickle files with sanitized traffic sign data here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Here are the steps you'll take to build the network:\n",
    "\n",
    "1. First load the data.\n",
    "2. Build a feedforward neural network to classify traffic signs.\n",
    "3. Build a convolutional neural network to classify traffic signs.\n",
    "\n",
    "Keep an eye on the networkâ€™s accuracy over time. Once the accuracy reaches the 98% range, you can be confident that youâ€™ve built and trained an effective model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Start by importing the data from the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# TODO: Implement load the data here.\n",
    "# Load pickled data\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# TODO: fill this in based on where you saved the training and testing data\n",
    "def get_data(tr, te):\n",
    "    #training_file = \"train.p\"\n",
    "    #testing_file = \"test.p\"\n",
    "    training_file = tr\n",
    "    testing_file = te\n",
    "    with open(training_file, mode='rb') as f:\n",
    "\n",
    "        train = pickle.load(f)\n",
    "\n",
    "    with open(testing_file, mode='rb') as f:\n",
    "\n",
    "        test = pickle.load(f)\n",
    "\n",
    "        X_train, y_train = train['features'], train['labels']\n",
    "\n",
    "        X_test, y_test = test['features'], test['labels']\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_data(\"train.p\",\"test.p\")\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "assert(X_train.shape[0] == y_train.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "assert(X_train.shape[1:] == (32,32,3)), \"The dimensions of the images are not 32 x 32 x 3.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data\n",
    "\n",
    "Now that you've loaded the training data, normalize the input so that it has a mean of 0 and a range between -0.5 and 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Implement data normalization here.\n",
    "def normalize_set(image_set, output_flat =True):\n",
    "    output_set=[]   \n",
    "    for i in range(0, image_set.shape[0]):\n",
    "        r_norm=np.array([])\n",
    "        r,g,b = cv2.split(image_set[i])        \n",
    "        r_flat, g_flat, b_flat = r.flatten(), g.flatten(), b.flatten()\n",
    "\n",
    "        r_norm = cv2.normalize(r_flat, r_flat, alpha=-0.5, beta=0.5, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        g_norm = cv2.normalize(g_flat,g_flat, alpha=-0.5, beta=0.5, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        b_norm = cv2.normalize(b_flat,b_flat, alpha=-0.5, beta=0.5, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "\n",
    "        if(output_flat):\n",
    "            norm_extra_bracket = cv2.merge((r_norm,  g_norm, b_norm ))\n",
    "            #print(norm_extra_bracket.shape,\"f\")\n",
    "            norm = norm_extra_bracket[:,0,:]\n",
    "            #print(norm.shape,\"f_no_extra_bracket\")          \n",
    "            \n",
    "        else:\n",
    "            r_norm32, g_norm32, b_norm32 =np.reshape(r_norm, (32,32)), np.reshape(g_norm, (32,32)),np.reshape(b_norm, (32,32))\n",
    "            norm  = cv2.merge((r_norm32, g_norm32, b_norm32))\n",
    "        output_set.append(norm)\n",
    "\n",
    "    return np.array(output_set)  \n",
    " \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "print(type(X_train))\n",
    "print(X_train.shape) \n",
    "print(type(X_test))\n",
    "print(X_test.shape) \n",
    "\n",
    "\n",
    "\n",
    "#print(X_train[0])\n",
    "#print(type(X_train), 'b')\n",
    "#print(X_train.shape, 'b')\n",
    "#print(X_train[0])\n",
    "\n",
    "\n",
    "#X_train =normalize_set(X_train, True)\n",
    "X_train =normalize_set(X_train, True)\n",
    "X_test =normalize_set(X_test, True)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "assert(round(np.mean(X_train)) == 0), \"The mean of the input data is: %f\" % np.mean(X_train)\n",
    "assert(np.min(X_train) == -0.5 and np.max(X_train) == 0.5), \"The range of the input data is: %.1f to %.1f\" % (np.min(X_train), np.max(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Two-Layer Feedfoward Network\n",
    "\n",
    "The code you've written so far is for data processing, not specific to Keras. Here you're going to build Keras-specific code.\n",
    "\n",
    "Build a two-layer feedforward neural network, with 128 neurons in the fully-connected hidden layer. \n",
    "\n",
    "To get started, review the Keras documentation about [models](https://keras.io/models/sequential/) and [layers](https://keras.io/layers/core/).\n",
    "\n",
    "The Keras example of a [Multi-Layer Perceptron](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py) network is similar to what you need to do here. Use that as a guide, but keep in mind that there are a number of differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Build a two-layer feedforward neural network with Keras here.\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch_size = 128\n",
    "batch_size = 128\n",
    "nb_classes = 43\n",
    "nb_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#below is to normalize that was done above.\n",
    "#X_train /= 255\n",
    "#X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices !!!! changes to capital Y\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "\n",
    "def flatten_set(image_set):\n",
    "    output_set=[]   \n",
    "    for i in range(0, image_set.shape[0]):\n",
    "\n",
    "        flat = image_set[i].flatten()\n",
    "        output_set.append(flat)\n",
    "\n",
    "    return np.array(output_set) \n",
    "X_train=flatten_set(X_train)\n",
    "X_test=flatten_set(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "#Build the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_shape=(3072, ), name='mac_and_cheese'))\n",
    "#32*32*3=3072\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(43, name= 'output'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "#assert(model.get_layer(name=\"hidden1\").input_shape == (None, 32*32*3)), \"The input shape is: %s\" % model.get_layer(name=\"hidden1\").input_shape\n",
    "assert(model.get_layer(name=\"output\").output_shape == (None, 43)), \"The output shape is: %s\" % model.get_layer(name=\"output\").output_shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "Compile and train the network for 2 epochs. [Use the `adam` optimizer, with `categorical_crossentropy` loss.](https://keras.io/models/sequential/)\n",
    "\n",
    "Hint 1: In order to use categorical cross entropy, you will need to [one-hot encode the labels](https://github.com/fchollet/keras/blob/master/keras/utils/np_utils.py).\n",
    "\n",
    "Hint 2: In order to pass the input images to the fully-connected hidden layer, you will need to [reshape the input](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py).\n",
    "\n",
    "Hint 3: Keras's `.fit()` method returns a `History.history` object, which the tests below use. Save that to a variable named `history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Compile and train the model here.\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "assert(history.history['acc'][0] > 0.5), \"The training accuracy was: %.3f\" % history.history['acc']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Network\n",
    "Split the training data into a training and validation set.\n",
    "\n",
    "Measure the [validation accuracy](https://keras.io/models/sequential/) of the network after two training epochs.\n",
    "\n",
    "Hint: [Use the `train_test_split()` method](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Split some of the training data into a validation dataset.\n",
    "# TODO: Compile and train the model to measure validation accuracy.\n",
    "# Get randomized datasets for training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    \n",
    "    X_train,\n",
    "    \n",
    "    Y_train,\n",
    "    \n",
    "    test_size=0.25,\n",
    "    \n",
    "    random_state=832289)\n",
    "\n",
    "\n",
    "#print('Training features and labels randomized and split.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_val= X_val.astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_val, Y_val))\n",
    "score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print('Val score:', score[0])\n",
    "print('Val accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "      \n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "assert(round(X_train.shape[0] / float(X_val.shape[0])) == 3), \"The training set is %.3f times larger than the validation set.\" % X_train.shape[0] / float(X_val.shape[0])\n",
    "assert(history.history['val_acc'][0] > 0.6), \"The validation accuracy is: %.3f\" % history.history['val_acc'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Accuracy**: (87%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations\n",
    "You've built a feedforward neural network in Keras!\n",
    "\n",
    "Don't stop here! Next, you'll add a convolutional layer to drive.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "Build a new network, similar to your existing network. Before the hidden layer, add a 3x3 [convolutional layer](https://keras.io/layers/convolutional/#convolution2d) with 32 filters and valid padding.\n",
    "\n",
    "Then compile and train the network.\n",
    "\n",
    "Hint 1: The Keras example of a [convolutional neural network](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) for MNIST would be a good example to review.\n",
    "\n",
    "Hint 2: Now that the first layer of the network is a convolutional layer, you no longer need to reshape the input images before passing them to the network. You might need to reload your training data to recover the original shape.\n",
    "\n",
    "Hint 3: Add a [`Flatten()` layer](https://keras.io/layers/core/#flatten) between the convolutional layer and the fully-connected hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Re-construct the network and add a convolutional layer before the first fully-connected layer.\n",
    "# TODO: Compile and train the model.\n",
    "\n",
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# TODO: Implement load the data here.\n",
    "# Load pickled data\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# TODO: fill this in based on where you saved the training and testing data\n",
    "def get_data(tr, te):\n",
    "    #training_file = \"train.p\"\n",
    "    #testing_file = \"test.p\"\n",
    "    training_file = tr\n",
    "    testing_file = te\n",
    "    with open(training_file, mode='rb') as f:\n",
    "\n",
    "        train = pickle.load(f)\n",
    "\n",
    "    with open(testing_file, mode='rb') as f:\n",
    "\n",
    "        test = pickle.load(f)\n",
    "\n",
    "        X_train, y_train = train['features'], train['labels']\n",
    "\n",
    "        X_test, y_test = test['features'], test['labels']\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "#X_train, y_train, X_test, y_test = get_data(\"train.p\",\"test.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "#nb_classes = 10\n",
    "nb_classes = 43\n",
    "nb_epoch = 12\n",
    "\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "\n",
    "#added for final play ground\n",
    "kernel_size_2 = (5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#from above\n",
    "X_train, y_train, X_test, y_test = get_data(\"train.p\",\"test.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(39209, 32, 32, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(12630, 32, 32, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(39209, 32, 32, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(12630, 32, 32, 3)\n",
      "(39209, 32, 32, 3)\n",
      "(12630, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(X_train.shape) \n",
    "print(type(X_test))\n",
    "print(X_test.shape) \n",
    "\n",
    "\n",
    "def normalize_set(image_set, output_flat =True):\n",
    "    output_set=[]   \n",
    "    for i in range(0, image_set.shape[0]):\n",
    "        r_norm=np.array([])\n",
    "        r,g,b = cv2.split(image_set[i])        \n",
    "        r_flat, g_flat, b_flat = r.flatten(), g.flatten(), b.flatten()\n",
    "\n",
    "        r_norm = cv2.normalize(r_flat, r_flat, alpha=-0.5, beta=0.5, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        g_norm = cv2.normalize(g_flat,g_flat, alpha=-0.5, beta=0.5, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        b_norm = cv2.normalize(b_flat,b_flat, alpha=-0.5, beta=0.5, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "\n",
    "        if(output_flat):\n",
    "            norm_extra_bracket = cv2.merge((r_norm,  g_norm, b_norm ))\n",
    "            #print(norm_extra_bracket.shape,\"f\")\n",
    "            norm = norm_extra_bracket[:,0,:]\n",
    "            #print(norm.shape,\"f_no_extra_bracket\")          \n",
    "            \n",
    "        else:\n",
    "            r_norm32, g_norm32, b_norm32 =np.reshape(r_norm, (32,32)), np.reshape(g_norm, (32,32)),np.reshape(b_norm, (32,32))\n",
    "            norm  = cv2.merge((r_norm32, g_norm32, b_norm32))\n",
    "        output_set.append(norm)\n",
    "\n",
    "    return np.array(output_set)  \n",
    "\n",
    "print(type(X_train))\n",
    "print(X_train.shape) \n",
    "print(type(X_test))\n",
    "print(X_test.shape) \n",
    "\n",
    "#X_train =normalize_set(X_train, True)\n",
    "X_train =normalize_set(X_train, False)\n",
    "X_test =normalize_set(X_test, False)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf\n"
     ]
    }
   ],
   "source": [
    "print(K.image_dim_ordering())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (39209, 32, 32, 3)\n",
      "39209 train samples\n",
      "12630 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "#max min normalier used above\n",
    "#X_train /= 255\n",
    "#X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Split some of the training data into a validation dataset.\n",
    "# TODO: Compile and train the model to measure validation accuracy.\n",
    "# Get randomized datasets for training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    \n",
    "    X_train,\n",
    "    \n",
    "    Y_train,\n",
    "    \n",
    "    test_size=0.3,\n",
    "    \n",
    "    random_state=832289)\n",
    "\n",
    "\n",
    "#print('Training features and labels randomized and split.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 30, 30, 32)    896         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 30, 30, 32)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 28, 28, 32)    9248        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 28, 28, 32)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25088)         0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           3211392     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 43)            5547        activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 43)            0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3227083\n",
      "____________________________________________________________________________________________________\n",
      "Train on 27446 samples, validate on 11763 samples\n",
      "Epoch 1/12\n",
      "27446/27446 [==============================] - 90s - loss: 0.6171 - acc: 0.8335 - val_loss: 0.1338 - val_acc: 0.9636\n",
      "Epoch 2/12\n",
      "27446/27446 [==============================] - 87s - loss: 0.0719 - acc: 0.9820 - val_loss: 0.0838 - val_acc: 0.9781\n",
      "Epoch 3/12\n",
      "27446/27446 [==============================] - 86s - loss: 0.0298 - acc: 0.9925 - val_loss: 0.0713 - val_acc: 0.9819\n",
      "Epoch 4/12\n",
      "27446/27446 [==============================] - 86s - loss: 0.0133 - acc: 0.9965 - val_loss: 0.0766 - val_acc: 0.9800\n",
      "Epoch 5/12\n",
      "27446/27446 [==============================] - 85s - loss: 0.0051 - acc: 0.9992 - val_loss: 0.0716 - val_acc: 0.9852\n",
      "Epoch 6/12\n",
      "27446/27446 [==============================] - 86s - loss: 0.0026 - acc: 0.9998 - val_loss: 0.0718 - val_acc: 0.9855\n",
      "Epoch 7/12\n",
      "27446/27446 [==============================] - 89s - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0736 - val_acc: 0.9860\n",
      "Epoch 8/12\n",
      "27446/27446 [==============================] - 84s - loss: 6.7693e-04 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9863\n",
      "Epoch 9/12\n",
      "27446/27446 [==============================] - 88s - loss: 6.2764e-04 - acc: 1.0000 - val_loss: 0.0811 - val_acc: 0.9857\n",
      "Epoch 10/12\n",
      "27446/27446 [==============================] - 87s - loss: 6.1152e-04 - acc: 1.0000 - val_loss: 0.0825 - val_acc: 0.9860\n",
      "Epoch 11/12\n",
      "27446/27446 [==============================] - 93s - loss: 6.0235e-04 - acc: 1.0000 - val_loss: 0.0823 - val_acc: 0.9870\n",
      "Epoch 12/12\n",
      "27446/27446 [==============================] - 90s - loss: 5.9836e-04 - acc: 1.0000 - val_loss: 0.0846 - val_acc: 0.9864\n",
      "Val score: 0.0845612710822\n",
      "Val accuracy: 0.986398027714\n",
      "Test score: 0.57650026924\n",
      "Test accuracy: 0.918289786214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#conctruct\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=pool_size))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#compile and train\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_val, Y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print('Val score:', score[0])\n",
    "print('Val accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "#assert(history.history['val_acc'][0] > 0.9), \"The validation accuracy is: %.3f\" % history.history['val_acc'][0]\n",
    "#below is correct above incorrectly looks at the 'val_acc' after the first epoch. A bug report has been filed. \n",
    "#This may be fixed in your version.\n",
    "assert(history.history['val_acc'][-1] > 0.9), \"The validation accuracy is: %.3f\" % history.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Accuracy**: (98.6%)\n",
    "**Test Accuracy**: (91.8%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "Re-construct your network and add a 2x2 [pooling layer](https://keras.io/layers/pooling/#maxpooling2d) immediately following your convolutional layer.\n",
    "\n",
    "Then compile and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 30, 30, 32)    896         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 30, 30, 32)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 28, 28, 32)    9248        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 28, 28, 32)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 14, 14, 32)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 6272)          0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           802944      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 43)            5547        activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 43)            0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 818635\n",
      "____________________________________________________________________________________________________\n",
      "Train on 27446 samples, validate on 11763 samples\n",
      "Epoch 1/12\n",
      "27446/27446 [==============================] - 57s - loss: 0.7558 - acc: 0.7906 - val_loss: 0.1442 - val_acc: 0.9608\n",
      "Epoch 2/12\n",
      "27446/27446 [==============================] - 59s - loss: 0.0934 - acc: 0.9765 - val_loss: 0.0850 - val_acc: 0.9793\n",
      "Epoch 3/12\n",
      "27446/27446 [==============================] - 58s - loss: 0.0475 - acc: 0.9883 - val_loss: 0.0682 - val_acc: 0.9819\n",
      "Epoch 4/12\n",
      "27446/27446 [==============================] - 58s - loss: 0.0256 - acc: 0.9944 - val_loss: 0.0727 - val_acc: 0.9806\n",
      "Epoch 5/12\n",
      "27446/27446 [==============================] - 63s - loss: 0.0151 - acc: 0.9968 - val_loss: 0.0582 - val_acc: 0.9863\n",
      "Epoch 6/12\n",
      "27446/27446 [==============================] - 61s - loss: 0.0093 - acc: 0.9982 - val_loss: 0.0594 - val_acc: 0.9859\n",
      "Epoch 7/12\n",
      "27446/27446 [==============================] - 61s - loss: 0.0058 - acc: 0.9989 - val_loss: 0.0586 - val_acc: 0.9867\n",
      "Epoch 8/12\n",
      "27446/27446 [==============================] - 63s - loss: 0.0033 - acc: 0.9995 - val_loss: 0.0561 - val_acc: 0.9878\n",
      "Epoch 9/12\n",
      "27446/27446 [==============================] - 64s - loss: 0.0020 - acc: 0.9997 - val_loss: 0.0565 - val_acc: 0.9883\n",
      "Epoch 10/12\n",
      "27446/27446 [==============================] - 66s - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0589 - val_acc: 0.9880\n",
      "Epoch 11/12\n",
      "27446/27446 [==============================] - 66s - loss: 9.4119e-04 - acc: 0.9999 - val_loss: 0.0593 - val_acc: 0.9884\n",
      "Epoch 12/12\n",
      "27446/27446 [==============================] - 64s - loss: 6.7507e-04 - acc: 1.0000 - val_loss: 0.0612 - val_acc: 0.9889\n",
      "Val score: 0.0612487306866\n",
      "Val accuracy: 0.988948397518\n",
      "Test score: 0.460281772778\n",
      "Test accuracy: 0.923911322211\n"
     ]
    }
   ],
   "source": [
    "# TODO: Re-construct the network and add a pooling layer after the convolutional layer.\n",
    "# TODO: Compile and train the model.\n",
    "\n",
    "\n",
    "\n",
    "#conctruct\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#compile and train\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_val, Y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print('Val score:', score[0])\n",
    "print('Val accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "assert(history.history['val_acc'][-1] > 0.9), \"The validation accuracy is: %.3f\" % history.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Accuracy**: (98.9%)\n",
    "**Test Accuracy: (92.4%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "Re-construct your network and add [dropout](https://keras.io/layers/core/#dropout) after the pooling layer. Set the dropout rate to 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 30, 30, 32)    896         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 30, 30, 32)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 28, 28, 32)    9248        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 28, 28, 32)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 14, 14, 32)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 14, 14, 32)    0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 6272)          0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           802944      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 128)           0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 43)            5547        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 43)            0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 818635\n",
      "____________________________________________________________________________________________________\n",
      "Train on 27446 samples, validate on 11763 samples\n",
      "Epoch 1/12\n",
      "27446/27446 [==============================] - 59s - loss: 1.3464 - acc: 0.6270 - val_loss: 0.2635 - val_acc: 0.9413\n",
      "Epoch 2/12\n",
      "27446/27446 [==============================] - 58s - loss: 0.4151 - acc: 0.8735 - val_loss: 0.1327 - val_acc: 0.9717\n",
      "Epoch 3/12\n",
      "27446/27446 [==============================] - 58s - loss: 0.2654 - acc: 0.9211 - val_loss: 0.0860 - val_acc: 0.9795\n",
      "Epoch 4/12\n",
      "27446/27446 [==============================] - 58s - loss: 0.1966 - acc: 0.9409 - val_loss: 0.0740 - val_acc: 0.9814\n",
      "Epoch 5/12\n",
      "27446/27446 [==============================] - 59s - loss: 0.1537 - acc: 0.9536 - val_loss: 0.0574 - val_acc: 0.9848\n",
      "Epoch 6/12\n",
      "27446/27446 [==============================] - 59s - loss: 0.1332 - acc: 0.9608 - val_loss: 0.0498 - val_acc: 0.9872\n",
      "Epoch 7/12\n",
      "27446/27446 [==============================] - 59s - loss: 0.1161 - acc: 0.9662 - val_loss: 0.0443 - val_acc: 0.9888\n",
      "Epoch 8/12\n",
      "27446/27446 [==============================] - 60s - loss: 0.0992 - acc: 0.9710 - val_loss: 0.0417 - val_acc: 0.9893\n",
      "Epoch 9/12\n",
      "27446/27446 [==============================] - 60s - loss: 0.0859 - acc: 0.9741 - val_loss: 0.0390 - val_acc: 0.9903\n",
      "Epoch 10/12\n",
      "27446/27446 [==============================] - 59s - loss: 0.0792 - acc: 0.9764 - val_loss: 0.0402 - val_acc: 0.9892\n",
      "Epoch 11/12\n",
      "27446/27446 [==============================] - 60s - loss: 0.0727 - acc: 0.9774 - val_loss: 0.0338 - val_acc: 0.9914\n",
      "Epoch 12/12\n",
      "27446/27446 [==============================] - 62s - loss: 0.0657 - acc: 0.9804 - val_loss: 0.0338 - val_acc: 0.9911\n",
      "Val score: 0.0338258616836\n",
      "Val accuracy: 0.991073705687\n",
      "Test score: 0.181041314308\n",
      "Test accuracy: 0.953681710204\n"
     ]
    }
   ],
   "source": [
    "# TODO: Re-construct the network and add dropout after the pooling layer.\n",
    "# TODO: Compile and train the model.\n",
    "\n",
    "#conctruct\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#compile and train\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_val, Y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print('Val score:', score[0])\n",
    "print('Val accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "assert(history.history['val_acc'][-1] > 0.9), \"The validation accuracy is: %.3f\" % history.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "**Validation Accuracy**: (99.1%)\n",
    "**Test Accuracy: (95.4%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Congratulations! You've built a neural network with convolutions, pooling, dropout, and fully-connected layers, all in just a few lines of code.\n",
    "\n",
    "Have fun with the model and see how well you can do! Add more layers, or regularization, or different padding, or batches, or more training epochs.\n",
    "\n",
    "What is the best validation accuracy you can achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_4 (Convolution2D)  (None, 30, 28, 32)    1472        convolution2d_input_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 30, 28, 32)    0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 28, 24, 32)    15392       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 28, 24, 32)    0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 26, 20, 32)    15392       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 26, 20, 32)    0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 13, 10, 32)    0           activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 13, 10, 32)    0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 4160)          0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 30, 30, 32)    896         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 30, 30, 32)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 28, 28, 32)    9248        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 28, 28, 32)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 28, 28, 32)    9248        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 28, 28, 32)    0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 14, 14, 32)    0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 14, 14, 32)    0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 6272)          0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           1335424     merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 128)           0           activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 43)            5547        dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 43)            0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1392619\n",
      "____________________________________________________________________________________________________\n",
      "Train on 27446 samples, validate on 11763 samples\n",
      "Epoch 1/12\n",
      "27446/27446 [==============================] - 195s - loss: 1.1933 - acc: 0.6766 - val_loss: 0.1757 - val_acc: 0.9576\n",
      "Epoch 2/12\n",
      "27446/27446 [==============================] - 194s - loss: 0.2933 - acc: 0.9141 - val_loss: 0.0862 - val_acc: 0.9792\n",
      "Epoch 3/12\n",
      "27446/27446 [==============================] - 202s - loss: 0.1723 - acc: 0.9494 - val_loss: 0.0626 - val_acc: 0.9838\n",
      "Epoch 4/12\n",
      "27446/27446 [==============================] - 203s - loss: 0.1241 - acc: 0.9634 - val_loss: 0.0445 - val_acc: 0.9882\n",
      "Epoch 5/12\n",
      "27446/27446 [==============================] - 204s - loss: 0.0992 - acc: 0.9706 - val_loss: 0.0362 - val_acc: 0.9908\n",
      "Epoch 6/12\n",
      "27446/27446 [==============================] - 205s - loss: 0.0786 - acc: 0.9775 - val_loss: 0.0330 - val_acc: 0.9906\n",
      "Epoch 7/12\n",
      "27446/27446 [==============================] - 205s - loss: 0.0679 - acc: 0.9803 - val_loss: 0.0285 - val_acc: 0.9922\n",
      "Epoch 8/12\n",
      "27446/27446 [==============================] - 221s - loss: 0.0593 - acc: 0.9825 - val_loss: 0.0296 - val_acc: 0.9910\n",
      "Epoch 9/12\n",
      "27446/27446 [==============================] - 247s - loss: 0.0510 - acc: 0.9849 - val_loss: 0.0268 - val_acc: 0.9929\n",
      "Epoch 10/12\n",
      "27446/27446 [==============================] - 247s - loss: 0.0483 - acc: 0.9857 - val_loss: 0.0244 - val_acc: 0.9930\n",
      "Epoch 11/12\n",
      "27446/27446 [==============================] - 246s - loss: 0.0402 - acc: 0.9882 - val_loss: 0.0230 - val_acc: 0.9942\n",
      "Epoch 12/12\n",
      "27446/27446 [==============================] - 247s - loss: 0.0375 - acc: 0.9885 - val_loss: 0.0283 - val_acc: 0.9930\n",
      "Val score: 0.0283412552275\n",
      "Val accuracy: 0.993028989203\n",
      "Test score: 0.168438842645\n",
      "Test accuracy: 0.959699129048\n"
     ]
    }
   ],
   "source": [
    "#already done above\n",
    "\n",
    "#with open('./test.p', mode='rb') as f:\n",
    "#    test = pickle.load(f)\n",
    "#X_test = test['features']\n",
    "#y_test = test['labels']\n",
    "#X_test = X_test.astype('float32')\n",
    "#X_test /= 255\n",
    "#X_test -= 0.5\n",
    "#Y_test = np_utils.to_categorical(y_test, 43)\n",
    "\n",
    "\n",
    "#Construct model\n",
    "#Construct right branch\n",
    "from keras.layers import Merge\n",
    "\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "right_branch.add(Activation('relu'))\n",
    "right_branch.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "right_branch.add(Activation('relu'))\n",
    "right_branch.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1], border_mode='same'))\n",
    "right_branch.add(Activation('relu'))\n",
    "right_branch.add(MaxPooling2D(pool_size=pool_size))\n",
    "right_branch.add(Dropout(0.25))\n",
    "right_branch.add(Flatten())\n",
    "\n",
    "\n",
    "#Construct left branch\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Convolution2D(nb_filters, kernel_size[0], kernel_size_2[1],\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "left_branch.add(Activation('relu'))\n",
    "left_branch.add(Convolution2D(nb_filters, kernel_size[0], kernel_size_2[1]))\n",
    "left_branch.add(Activation('relu'))\n",
    "left_branch.add(Convolution2D(nb_filters, kernel_size[0], kernel_size_2[1]))\n",
    "left_branch.add(Activation('relu'))\n",
    "left_branch.add(MaxPooling2D(pool_size=pool_size))\n",
    "left_branch.add(Dropout(0.25))\n",
    "left_branch.add(Flatten())\n",
    "\n",
    "\n",
    "\n",
    "#Merge branches and construct final model\n",
    "merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "\n",
    "final_model.add(Dense(128))\n",
    "final_model.add(Activation('relu'))\n",
    "final_model.add(Dropout(0.5))\n",
    "final_model.add(Dense(nb_classes))\n",
    "final_model.add(Activation('softmax'))\n",
    "\n",
    "final_model.summary()\n",
    "\n",
    "#compile and train\n",
    "final_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = final_model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=([X_val,X_val], Y_val))\n",
    "\n",
    "score = final_model.evaluate([X_val,X_val], Y_val, verbose=0)\n",
    "print('Val score:', score[0])\n",
    "print('Val accuracy:', score[1])\n",
    "\n",
    "score = final_model.evaluate([X_test, X_test], Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "assert(history.history['val_acc'][-1] > 0.9), \"The validation accuracy is: %.3f\" % history.history['val_acc'][-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Accuracy**: (99.3%)\n",
    "**Test Accuracy: (96.0%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Once you've picked out your best model, it's time to test it.\n",
    "\n",
    "Load up the test data and use the [`evaluate()` method](https://keras.io/models/model/#evaluate) to see how well it does.\n",
    "\n",
    "Hint 1: After you load your test data, don't forget to normalize the input and one-hot encode the output, so it matches the training data.\n",
    "\n",
    "Hint 2: The `evaluate()` method should return an array of numbers. Use the `metrics_names()` method to get the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.168438842645\n",
      "Test accuracy: 0.959699129048\n"
     ]
    }
   ],
   "source": [
    "score = final_model.evaluate([X_test, X_test], Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "assert(history.history['val_acc'][-1] > 0.9), \"The validation accuracy is: %.3f\" % history.history['val_acc'][-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Accuracy: (96.0%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Keras is a great tool to use if you want to quickly build a neural network and evaluate performance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python3Environment]",
   "language": "python",
   "name": "Python [python3Environment]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
