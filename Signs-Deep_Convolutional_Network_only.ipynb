{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Exploration\n",
    "\n",
    "Visualize the German Traffic Signs Dataset. This is open ended, some suggestions include: plotting traffic signs images, plotting the count of each sign, etc. Be creative!\n",
    "\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- features -> the images pixel values, (width, height, channels)\n",
    "- labels -> the label of the traffic sign\n",
    "- sizes -> the original width and height of the image, (width, height)\n",
    "- coords -> coordinates of a bounding box around the sign in the image, (x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load pickled data\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# TODO: fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = \"traffic-signs-data.zip/train.p\"\n",
    "\n",
    "testing_file = \"traffic-signs-data.zip/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    \n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "def rbg_to_gray(array):\n",
    "    \n",
    "        #fs for features\n",
    "\n",
    "        fs = []\n",
    "\n",
    "        for i in range(0,array.shape[0]):\n",
    "\n",
    "            image = array[i]\n",
    "\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            fs.append(gray_image)\n",
    "            \n",
    "        return np.array(fs)\n",
    "\n",
    "\n",
    "X_train = rbg_to_gray(X_train)\n",
    "\n",
    "X_test = rbg_to_gray(X_test)\n",
    "\n",
    "#this is for testing 5 images later\n",
    "#my_test_X =rbg_to_gray(my_test_X)\n",
    "\n",
    "print (X_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 39209\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32) 1024\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "### To start off let's do a basic data summary.\n",
    "\n",
    "# TODO: number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: number of testing examples\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# TODO: number of testing examples\n",
    "#this is for later\n",
    "#n_my_test = my_test_X.shape[0]\n",
    "\n",
    "# TODO: what's the shape of an image?\n",
    "image_shape = X_train.shape[1:3]\n",
    "\n",
    "# TODO: how many classes are in the dataset\n",
    "n_classes = len(set(train['labels']))\n",
    "# TODO: how many classes are in the dataset\n",
    "n_classes2 = len(set(test['labels']))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "#this is for later\n",
    "#print(\"Number of my_test examples =\", n_my_test)\n",
    "print(\"Image data shape =\", image_shape, 32*32)\n",
    "print(\"Number of classes =\", n_classes, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "### Preprocess the data here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "print('All modules imported.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def flatten_gray(array):\n",
    "    \n",
    "        #fs for features\n",
    "\n",
    "        fs = []\n",
    "        \n",
    "        for i in range(0,array.shape[0]):\n",
    "        \n",
    "            image = array[i]\n",
    "\n",
    "            f = np.array(image, dtype=np.float32).flatten()\n",
    "\n",
    "            fs.append(f)\n",
    "\n",
    "        return np.array(fs) \n",
    "    \n",
    "X_train = flatten_gray(X_train)\n",
    "\n",
    "X_test = flatten_gray(X_test)\n",
    "\n",
    "#This is for later\n",
    "#my_test_X =flatten_gray(my_test_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_greyscale(image_data):\n",
    "    \n",
    "    a= 0.1\n",
    "    \n",
    "    b= 0.9\n",
    "    \n",
    "    min = 0\n",
    "    \n",
    "    max =255\n",
    "    \n",
    "    #value = 0.1 + ((image_data- min)*(b-a))/(max-min)\n",
    "\n",
    "    greyscale_min = 0\n",
    "\n",
    "    greyscale_max = 255\n",
    "\n",
    "    return a + ( ( (image_data - greyscale_min)*(b - a) )/( greyscale_max - greyscale_min ) )\n",
    "\n",
    "X_train = normalize_greyscale(X_train)\n",
    "\n",
    "X_test = normalize_greyscale(X_test)\n",
    "\n",
    "#This is for later\n",
    "#my_test_X = normalize_greyscale(my_test_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Turn labels into numbers and apply One-Hot Encoding\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "encoder.fit(y_train)\n",
    "\n",
    "y_train = encoder.transform(y_train)\n",
    "\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "#my_test_y = encoder.transform(my_test_y)\n",
    "\n",
    "# Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "#my_test_y = my_test_y.astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get randomized datasets for training and validation\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    \n",
    "    X_train,\n",
    "    \n",
    "    y_train,\n",
    "    \n",
    "    test_size=0.15,\n",
    "    \n",
    "    random_state=832289)\n",
    "\n",
    "\n",
    "#print('Training features and labels randomized and split.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#None is the batch size\n",
    "x = tf.placeholder(tf.float32, shape=[None, 1024])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([1024,43]))\n",
    "b = tf.Variable(tf.zeros([43]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "#train_step = tf.train.GradientDescentOptimizer(0.09).minimize(cross_entropy)\n",
    "#batch_size=1000\n",
    "\n",
    "#for i in range(1000):\n",
    "    \n",
    "#  batch_start = i*batch_size\n",
    "#  batch_features = X_train[batch_start:batch_start + batch_size]\n",
    "#  batch_labels = y_train[batch_start:batch_start + batch_size]  \n",
    "    \n",
    "#  #print(X_train.shape)  \n",
    "    \n",
    "    \n",
    "#  #batch = mnist.train.next_batch(100)\n",
    "#  #print(batch)\n",
    "#  #train_step.run(feed_dict={x: batch[0], y_: batch[1]})  \n",
    "#  train_step.run(feed_dict={x: batch_features, y_: batch_labels})\n",
    "    \n",
    "    \n",
    "#correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "#print(accuracy.eval(feed_dict={x: X_test, y_: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,32,32,1])\n",
    "#x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#print(h_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "#print(h_pool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([8 * 8 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 43])\n",
    "b_fc2 = bias_variable([43])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(8.8e-4).minimize(cross_entropy)\n",
    "#above with 40bach\n",
    "#train_step = tf.train.AdamOptimizer(9e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start  15:29:45.518406\n",
      "step 0, training accuracy 0.025, batch start 0\n",
      "step 100, training accuracy 0.3, batch start 4000\n",
      "step 200, training accuracy 0.7, batch start 8000\n",
      "step 300, training accuracy 0.55, batch start 12000\n",
      "step 400, training accuracy 0.8, batch start 16000\n",
      "step 500, training accuracy 0.725, batch start 20000\n",
      "step 600, training accuracy 0.85, batch start 24000\n",
      "step 700, training accuracy 0.825, batch start 28000\n",
      "step 800, training accuracy 0.875, batch start 32000\n",
      "valid accuracy 0.893064\n",
      "step 0, training accuracy 0.9, batch start 0\n",
      "step 100, training accuracy 0.975, batch start 4000\n",
      "step 200, training accuracy 1, batch start 8000\n",
      "step 300, training accuracy 0.95, batch start 12000\n",
      "step 400, training accuracy 0.95, batch start 16000\n",
      "step 500, training accuracy 0.925, batch start 20000\n",
      "step 600, training accuracy 0.975, batch start 24000\n",
      "step 700, training accuracy 0.975, batch start 28000\n",
      "step 800, training accuracy 0.95, batch start 32000\n",
      "valid accuracy 0.960218\n",
      "step 0, training accuracy 0.975, batch start 0\n",
      "step 100, training accuracy 1, batch start 4000\n",
      "step 200, training accuracy 1, batch start 8000\n",
      "step 300, training accuracy 0.975, batch start 12000\n",
      "step 400, training accuracy 1, batch start 16000\n",
      "step 500, training accuracy 0.975, batch start 20000\n",
      "step 600, training accuracy 1, batch start 24000\n",
      "step 700, training accuracy 0.95, batch start 28000\n",
      "step 800, training accuracy 0.95, batch start 32000\n",
      "valid accuracy 0.969228\n",
      "step 0, training accuracy 1, batch start 0\n",
      "step 100, training accuracy 0.975, batch start 4000\n",
      "step 200, training accuracy 1, batch start 8000\n",
      "step 300, training accuracy 1, batch start 12000\n",
      "step 400, training accuracy 1, batch start 16000\n",
      "step 500, training accuracy 1, batch start 20000\n",
      "step 600, training accuracy 1, batch start 24000\n",
      "step 700, training accuracy 0.975, batch start 28000\n",
      "step 800, training accuracy 0.975, batch start 32000\n",
      "valid accuracy 0.981979\n",
      "step 0, training accuracy 1, batch start 0\n",
      "step 100, training accuracy 1, batch start 4000\n",
      "step 200, training accuracy 1, batch start 8000\n",
      "step 300, training accuracy 1, batch start 12000\n",
      "step 400, training accuracy 1, batch start 16000\n",
      "step 500, training accuracy 1, batch start 20000\n",
      "step 600, training accuracy 1, batch start 24000\n",
      "step 700, training accuracy 0.975, batch start 28000\n",
      "step 800, training accuracy 1, batch start 32000\n",
      "valid accuracy 0.981299\n",
      "test accuracy 0.917023\n",
      "Finished  15:37:55.326645\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "batch_size=50#40\n",
    "total_samples = X_train.shape[0]\n",
    "epochs = 5\n",
    "\n",
    "#Loops (at batch size) to get through data once.\n",
    "loops = int(total_samples / batch_size) \n",
    "\n",
    "print(\"Start \",datetime.datetime.now().time())\n",
    "\n",
    "for i in range(epochs):\n",
    "    #for j in range(800):\n",
    "    for j in range(loops):\n",
    "      #batch_size=40\n",
    "\n",
    "      #print(X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "      batch_start = j*batch_size\n",
    "      batch_features = X_train[batch_start:(batch_start + batch_size)]\n",
    "      batch_labels = y_train[batch_start:(batch_start + batch_size)] \n",
    "\n",
    "\n",
    "\n",
    "      #batch = mnist.train.next_batch(50)\n",
    "\n",
    "      #print(batch[0].shape, batch[1].shape)\n",
    "\n",
    "      if j%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch_features, y_: batch_labels, keep_prob: 1.0})\n",
    "            #x: X_train[0:50], y_: y_train[0:50], keep_prob: 1.0})  \n",
    "        print(\"step %d, training accuracy %g, batch start %d\"%(j, train_accuracy, batch_start))\n",
    "\n",
    "    #  print(batch_features)\n",
    "    #  print(type(batch_features))\n",
    "    #  print(batch_features.shape)\n",
    "\n",
    "    #  print(batch_labels)   \n",
    "    #  print(type(batch_labels))\n",
    "    #  print(batch_labels.shape)    \n",
    "\n",
    "\n",
    "      train_step.run(feed_dict={x: batch_features, y_: batch_labels, keep_prob: 0.5})\n",
    "\n",
    "    print(\"valid accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    #    x: X_test, y_: y_test, keep_prob: 1.0}))    \n",
    "\n",
    "        x: X_valid, y_: y_valid, keep_prob: 1.0}))\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={    \n",
    "    x: X_test, y_: y_test, keep_prob: 1.0}))    \n",
    "\n",
    "print (\"Finished \", datetime.datetime.now().time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is for later.\n",
    "#print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "#    x: my_test_X, y_: my_test_y, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "1 https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/index.html#train-the-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python3Environment]",
   "language": "python",
   "name": "Python [python3Environment]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
